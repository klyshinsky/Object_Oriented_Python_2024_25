{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dummy:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.__b = 1\n",
    "        self.c = \"2\"\n",
    "        d = None\n",
    "        print(\"It's me\")\n",
    "        \n",
    "    def print_b(self):\n",
    "        print(self.__b)\n",
    "        \n",
    "    def dummy_func(self, a: list) -> str:\n",
    "        return str(a)\n",
    "    \n",
    "    f: int = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's me\n",
      "It's me\n"
     ]
    }
   ],
   "source": [
    "a = Dummy()\n",
    "a2 = Dummy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[1]'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.dummy_func([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.__b = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "a.print_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = a.print_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "aaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "del a\n",
    "aaa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dummy' object has no attribute '__b'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_49575/2198712572.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dummy' object has no attribute '__b'"
     ]
    }
   ],
   "source": [
    "a.__b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_Dummy__b',\n",
       " '__annotations__',\n",
       " '__b',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'c',\n",
       " 'f',\n",
       " 'print_b']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "a._Dummy__b = 3\n",
    "a.print_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qwerty():\n",
    "    print(\"asdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwerty2 = qwerty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asdf\n"
     ]
    }
   ],
   "source": [
    "qwerty2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "method"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a.print_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.qqqqq = a.print_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "a.qqqqq()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Библиотека Typing\n",
    "\n",
    "Питон всё ещё считается языком без строгой типизации. Это значит, что для того, чтобы завести переменную не надо заранее указывать ее тип, а сам тип хранимого значения может поменяться по ходу работы программы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pymorphy3\n",
    "from pymorphy3.analyzer import MorphAnalyzer\n",
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 <class 'int'>\n",
      "1.0 <class 'float'>\n",
      "1 <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "a = 1\n",
    "print(a, type(a))\n",
    "a = 1.\n",
    "print(a, type(a))\n",
    "a = '1'\n",
    "print(a, type(a))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это серьезно упрощает разработку, не накладывая строгих ограничений на очень многие процессы. С другой стороны, программисту гораздо проще сделать ошибку. В примере ниже мы забыли, что `а` приняло значение строки. Язык со строгой типизацией отказался бы выполнять операцию, так как типы сравниваемых значений не совпадают, а Питон ничего, выполнил. Теперь проблема на нашей стороне.\n",
    "\n",
    "С другой стороны, подобное поведение избавляет нас от массы проблем, например, переписывать функцию несколько раз для различных типов данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It wents wrong.\n"
     ]
    }
   ],
   "source": [
    "if a == 1:\n",
    "    print('Correct')\n",
    "else:\n",
    "    print('It wents wrong.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако контроль типов - весьма полезная операция, отданная на откуп сторонним программам. Позже мы с вами познакомимся с линтерами, которые будут указывать на подобные ошибки. \n",
    "\n",
    "Мы уже увидели, что писать типы переменных - это хороший тон. Но пока мы моем писать только конкретный тип. Что делать, если конкретный тип неизвестен? Например, если в функцию может передаваться любой итерируемый объект?\n",
    "\n",
    "Для этого была разработана библиотека `typing`, которая хранит в себе объявления разных типов. Документация на библиотеку находится [здесь](https://docs.python.org/3/library/typing.html), дополнительные материалы в двух частях [здесь](https://habr.com/ru/company/lamoda/blog/432656/) и [здесь](https://habr.com/ru/company/lamoda/blog/435988/).\n",
    "\n",
    "Если какая-либо функция может не возвращать значение, или само значение может не передаваться в функцию, используется модификатор `Optional`. Теперь соответствующая функция, которая перебирает кодировки и возвращает `None`, если кодировка не была найдена, будет выглядеть вот так. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def checkAllEncodings(self) -> Optional[str]: # Теперь функция возвращает Optional[str], то есть str или None.\n",
    "    for encoding in self.allowedEncodings:\n",
    "        if checkEncoding(encoding):\n",
    "            return encoding\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если переменная, параметр или возвращаемое значение могут быть любого типа, следует использовать `Any`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current status is: 1\n",
      "Current status is: good\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "\n",
    "def print_status(status: Any) -> None:\n",
    "    print(f'Current status is: {str(status)}')\n",
    "    \n",
    "print_status(1)\n",
    "print_status('good')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы можем работать с ограниченным списком типов, используется модификатор `Union`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "from typing import Union\n",
    "\n",
    "def add_numbers(a: Union[int, float], b: Union[int, float]) -> Union[int, float]:\n",
    "    return a + b\n",
    "\n",
    "print(add_numbers(1, 1)) # int\n",
    "print(add_numbers(1., 1)) # float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для списков, кортежей и словарей можно использовать стандартные типы, но можно и типы из `typing`: `List, Tuple, Dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "\n",
    "def summ_items(data: Union[List, Tuple, Dict]) -> Union[int, float]:\n",
    "    return sum(data)\n",
    "\n",
    "print(summ_items([1, 2, 3, 4]))\n",
    "print(summ_items((1, 2, 3, 4)))\n",
    "print(summ_items({10:1, 20:2, 30:3, 40:4}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "def summ_items(data: Union[list, dict]) -> Union[int, float]:\n",
    "    return sum(data)\n",
    "\n",
    "print(summ_items([1, 2, 3, 4]))\n",
    "print(summ_items((1, 2, 3, 4)))\n",
    "print(summ_items({10:1, 20:2, 30:3, 40:4}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вообще, я хотел сказать, что в эту функцию можно передать любой итерируемый объект. Используем для этого `Iterator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# from collections.abc import Iterator\n",
    "from typing import Iterator\n",
    "\n",
    "def summ_items(data: Iterator) -> Iterator:\n",
    "    return sum(data)\n",
    "\n",
    "print(summ_items([1, 2, 3, 4]))\n",
    "print(summ_items((1, 2, 3, 4)))\n",
    "print(summ_items({10:1, 20:2, 30:3, 40:4}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы хотим показать, что в качестве типа используется функция или любой вызываемый объект, можно использовать `Callable`, у которого в квадратных скобках будет список передаваемых параметров и возвращаемое значение: `Callable[[ArgType1, ArgType2,...], ReturnType]`. Кстати, в соотетствующем классе я не прописал список свойств именно из-за того, что непонятно было, какой тип писать свойству-ссылке на функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FasterMorphologyUnified' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_49575/2151174476.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Ключевое слово class после которого идет название нашего класса.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mFasterMorphologyUnified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmorpho\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMorphAnalyzer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_49575/2151174476.py\u001b[0m in \u001b[0;36mFasterMorphologyUnified\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmorpho\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMorphAnalyzer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcash\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0manalyzeWords\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFasterMorphologyUnified\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m\"PyM\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Функция инициализации объекта после его создания.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FasterMorphologyUnified' is not defined"
     ]
    }
   ],
   "source": [
    "from typing import Callable\n",
    "\n",
    "# Ключевое слово class после которого идет название нашего класса.\n",
    "class FasterMorphologyUnified:\n",
    "    \n",
    "    morpho: MorphAnalyzer\n",
    "    cash: dict\n",
    "    analyzeWords: Callable[[FasterMorphologyUnified, list], list]\n",
    "        \n",
    "    def __init__(self, dict_type: str =\"PyM\") -> None: # Функция инициализации объекта после его создания.\n",
    "        # Создаем новую морфологию в каждом объекте. \n",
    "        # А вдруг мы будем потом работать с разными языками? У каждого объекта должна быть своя.\n",
    "        if dict_type == 'PyM':\n",
    "            self.morpho = pymorphy3.MorphAnalyzer() \n",
    "            self.cash = {} # Создаем словарь для кеширования.\n",
    "            self.analyzeWords = self.analyzeWordsWithPymorphy\n",
    "        elif dict_type == 'MyS':\n",
    "            self.mystem = pymystem3.Mystem()\n",
    "            self.analyzeWords = self.analyzeWordsWithMystem\n",
    "        # Вообще-то надо предусмотреть вариант, если нам передали какое-то еще значение, которого мы не знаем.\n",
    "        self.mode = dict_type # Сохраним, чтобы потом можно было понять что за словарь использовался.\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И вот тут возникает проблема. Мы не можем использовать имя класса внутри объявления этого класса, так как объявление ещё не закончено и новый тип не создан (а ну как всё закончится ошибкой?). В таком случае тип надо записать в виде строки, Питон тогда отложит поиск типа на потом.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "# Ключевое слово class после которого идет название нашего класса.\n",
    "class FasterMorphologyUnified:\n",
    "    \n",
    "    morpho: MorphAnalyzer\n",
    "    mystem: Mystem\n",
    "    cash: dict\n",
    "    analyzeWords: Callable[['FasterMorphologyUnified', 'list'], list]\n",
    "        \n",
    "    def __init__(self, dict_type: str =\"PyM\") -> None: # Функция инициализации объекта после его создания.\n",
    "        # Создаем новую морфологию в каждом объекте. \n",
    "        # А вдруг мы будем потом работать с разными языками? У каждого объекта должна быть своя.\n",
    "        if dict_type == 'PyM':\n",
    "            self.morpho = pymorphy3.MorphAnalyzer() \n",
    "            self.cash = {} # Создаем словарь для кеширования.\n",
    "            self.analyzeWords = self.analyzeWordsWithPymorphy\n",
    "        elif dict_type == 'MyS':\n",
    "            self.mystem = pymystem3.Mystem()\n",
    "            self.analyzeWords = self.analyzeWordsWithMystem\n",
    "        # Вообще-то надо предусмотреть вариант, если нам передали какое-то еще значение, которого мы не знаем.\n",
    "        self.mode = dict_type # Сохраним, чтобы потом можно было понять что за словарь использовался.\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Векторизация текстов</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь напишем класс, отвечающий за векторизацию текста."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для определения меры сходства двух текстов используется косинусная мера сходства, рассчитываемая по следующей формуле: $cos(a,b)=\\frac{\\sum{a_i * b_i}}{\\sqrt {\\sum{a_i^2}*\\sum{b_i^2}}}$.<br>\n",
    "Вообще-то, использовать стандартную функцию рассчета косинусной меры сходства из <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html\">sklearn</a> было бы быстрее. Но мне хотелось показать как работать с разными типами входа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Другое имя класса, так как он обладает несколько иной функциональностью.\n",
    "class FasterMorphology2:\n",
    "    \"\"\" Класс для быстрого морфологического анализа текстов и их векторизации. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self) -> None: # Функция инициализации объекта после его создания.\n",
    "        self.morpho = pymorphy3.MorphAnalyzer()\n",
    "        self.__cash = {}\n",
    "        self.__dictionary = {} # Добавим словарь для запоминания, на каком месте вектора находится какая начальная форма.\n",
    "        \n",
    "    def analyzeWords(self, words: list) -> list:\n",
    "        \"\"\" Проводит морфологический анализ списка токенов words.\n",
    "            Возвращает список начальных форм слов.\n",
    "        \"\"\"\n",
    "        res: list\n",
    "            \n",
    "        res = []\n",
    "        for w in words:\n",
    "            if w in self.__cash: # Сперва ищем очередное слово в кеше.\n",
    "                res.append(self.__cash[w])\n",
    "            else: # Если его там нет, проводим морфологический анализ и кешируем.\n",
    "                r = self.morpho.parse(w)[0].normal_form\n",
    "                res.append(r)\n",
    "                self.__cash[w] = r\n",
    "                if r not in self.__dictionary: # Также для каждой начальной формы запоминаем ее позицию в векторе.\n",
    "                    self.dictionary[r] = len(self.dictionary) + 1\n",
    "        return res\n",
    "    \n",
    "    def analyzeText(self, text: str) -> list:\n",
    "        \"\"\" Проводит морфологический анализ строки с текстом text. \n",
    "            Выделяет из нее слова, написанные русской кириллицей.\n",
    "            Возвращает список начальных форм слов.\n",
    "        \"\"\"\n",
    "        words: list\n",
    "        \n",
    "        words = [w[0] for w in re.findall(\"([А-ЯЁа-яё]+(-[А-ЯЁа-яё]+)*)\", text)]\n",
    "        return self.analyzeWords(words)\n",
    "        \n",
    "    # Вообще-то тоже самое умеет Counter, но ему надо сперва привести слова к начальной форме.\n",
    "    def vectorizeAsDict(self, words) -> dict:\n",
    "        \"\"\" Возвращает векторное разреженное представление текста в виде словаря.\n",
    "            Текст передается как список токенов words.\n",
    "            Вместо позиции для индексации используется само слово.\n",
    "            Возвращает словарь с начальными формами в ключах и частотами этих форм.\n",
    "        \"\"\"\n",
    "        vct: dict\n",
    "        res: list\n",
    "        \n",
    "        vct = {}\n",
    "        res = []\n",
    "        for w in words: # Для каждого слова проводим анализ.\n",
    "            if w in self.__cash:\n",
    "                vct[self.__cash[w]] = vct.get(self.__cash[w], 0) + 1 # Считаем частоты слов.\n",
    "            else:\n",
    "                r = self.morpho.parse(w)[0].normal_form\n",
    "                res.append(r)\n",
    "                self.__cash[w] = r\n",
    "                vct[r] = vct.get(r, 0) + 1\n",
    "                if r not in self.__dictionary:\n",
    "                    self.__dictionary[r] = len(self.__dictionary)\n",
    "        return vct\n",
    "    \n",
    "    def clearDict(self):\n",
    "        \"\"\" Очищает словарь. Вдруг надо пересчитать так как изменилась размерность пространства.\n",
    "        \"\"\"\n",
    "        self.dictionary = {}\n",
    "    \n",
    "    def formDict(self, texts: list):\n",
    "        \"\"\" Сформировать словарь по тексту не формируя разметку текста.\n",
    "        \"\"\"\n",
    "        for text in texts:\n",
    "            for word in text:\n",
    "                if word not in self.cash:\n",
    "                    r = self.morpho.parse(w)[0].normal_form\n",
    "                    self.__cash[word] = r\n",
    "                    if r not in self.__dictionary:\n",
    "                        self.__dictionary[r] = len(self.__dictionary)\n",
    "    \n",
    "    def vectorizeAsList(self, words: list) -> list:\n",
    "        \"\"\" Возвращает векторное представление текста в виде плотного списка (включает нули).\n",
    "            Текст передается как список токенов words.\n",
    "            Позиция каждого слова в векторе определяется числом, хранимым в dictionary.\n",
    "            Возвращает словарь с начальными формами в ключах и частотами этих форм.\n",
    "        \"\"\"\n",
    "        # Сперва обновляем dictionary.\n",
    "        for word in words:\n",
    "            if word not in self.__cash:\n",
    "                r = self.morpho.parse(w)[0].normal_form\n",
    "                self.__cash[word] = r\n",
    "                if r not in self.__dictionary:\n",
    "                    self.__dictionary[r]=len(self.__dictionary.keys())\n",
    "        # Теперь, когда все слова есть в кеше и словаре и известен размер вектора, можно приступать к векторизации.\n",
    "        vct = [0 for _ in self.__dictionary]\n",
    "        for word in words:\n",
    "            vct[self.__dictionary[self.__cash[word]]] += 1\n",
    "        return vct\n",
    "    \n",
    "    def vectorizeAsList2(self, words: list) -> list:\n",
    "        \"\"\" Возвращает векторное представление текста в виде плотного списка (включает нули).\n",
    "            Текст передается как список токенов words. В вектор включаются только слова, находящиес в словаре.\n",
    "            Позиция каждого слова в векторе определяется числом, хранимым в dictionary.\n",
    "            Возвращает словарь с начальными формами в ключах и частотами этих форм.\n",
    "        \"\"\"\n",
    "        vct = [0 for _ in self.__dictionary]\n",
    "        for word in words:\n",
    "            if word in self.__cash:\n",
    "                vct[self.__dictionary[self.__cash[word]]] += 1\n",
    "        return vct\n",
    "\n",
    "    def vectorizeAsArray(self, words: list) ->list:\n",
    "        \"\"\" Возвращает векторное представление текста в виде плотного массива (включает нули).\n",
    "            Текст передается как список токенов words.\n",
    "            Позиция каждого слова в векторе определяется числом, хранимым в dictionary.\n",
    "            Возвращает словарь с начальными формами в ключах и частотами этих форм.\n",
    "        \"\"\"\n",
    "        # Сперва обновляем dictionary.\n",
    "        for word in words:\n",
    "            if word not in self.__cash:\n",
    "                r = self.morpho.parse(w)[0].normal_form\n",
    "                self.cash[word] = r\n",
    "                if r not in self.__dictionary:\n",
    "                    self.__dictionary[r]=len(self.__dictionary.keys())\n",
    "        # Теперь, когда все слова есть в кеше и словаре и известен размер вектора, можно приступать к векторизации.\n",
    "        vct = np.zeros((len(self.__dictionary)))\n",
    "        for word in words:\n",
    "            vct[self.__dictionary[self.__cash[word]]] += 1\n",
    "        return vct\n",
    "\n",
    "    # Здесь мы заложили проблему. Функция не умеет считать расстояние между np.array.\n",
    "    # А ещё эти две функции не имеют никакого отношения к морфологическому анализу.\n",
    "    # Но представим себе, что мы завели ещё две функции, которые считают расстояние между текстами, а не векторами.\n",
    "    def cosineSimilarity(self, a, b) -> float:\n",
    "        \"\"\" Функция расчета косинусной меры сходства между двумя векторными представлениями текста.\n",
    "            Работает по-разному в зависимости от представления вектора.\n",
    "        \"\"\"\n",
    "        if type(a) != type(b): # Тип векторов должен совпадать.\n",
    "            return None\n",
    "        if isinstance(a, list): # Если это списки, значит это плотное представление вектора.\n",
    "            if 0 == len(a) or 0 == len(b) or len(a) != len(b): # Длины векторов в этом случае должны совпадать.\n",
    "                return 0\n",
    "            sumab = sum([a[na] * b[na] for na in range(len(a))])\n",
    "            suma2 = sum([a[na] * a[na] for na in range(len(a))])\n",
    "            sumb2 = sum([b[na] * b[na] for na in range(len(a))])\n",
    "            return sumab / math.sqrt(suma2 * sumb2)        \n",
    "        elif isinstance(a, dict): # Разреженное представление вектора - хранятся только ненулевые значения.\n",
    "            if 0 == len(a.keys()) or 0 == len(b.keys()): # Вектора должны хранить хоть что-то.\n",
    "                return 0\n",
    "            sumab = sum([a[na] * b[na] for na in set(a.keys()) & set(b.keys())])\n",
    "#            sumab=sum([a[na]*b[na] for na in a.keys() if na in b.keys()])\n",
    "            suma2 = sum([a[na] * a[na] for na in a.keys()])\n",
    "            sumb2 = sum([b[nb] * b[nb] for nb in b.keys()])\n",
    "            return sumab / math.sqrt(suma2 * sumb2)  \n",
    "        return 0\n",
    "    \n",
    "    def JaccardCoefficient(self, a, b) -> float:\n",
    "        \"\"\" Коэффициент Жаккара - отношение количества слов, встречающихся в обоих текстах к объединению лексики.\n",
    "        \"\"\"\n",
    "        if type(a) != type(b): # Тип векторов должен совпадать.\n",
    "            return None\n",
    "        if isinstance(a, list): # Если это списки, значит это плотное представление вектора.\n",
    "            if 0 == len(a) or 0 == len(b) or len(a) != len(b): # Длины векторов в этом случае должны совпадать.\n",
    "                return 0\n",
    "            union = len(a) - [aa * bb for aa, bb in zip(a, b)].count(0)\n",
    "            intersection = len(a) - [aa + bb for aa, bb in zip(a, b)].count(0)\n",
    "            return union / intersection\n",
    "        elif isinstance(a, dict): # Разреженное представление вектора - хранятся только ненулевые значения.\n",
    "            if 0 == len(a.keys()) or 0 == len(b.keys()): # Вектора должны хранить хоть что-то.\n",
    "                return 0\n",
    "            return len(set(a.keys()) & set(b.keys())) / len(set(a.keys()) | set(b.keys()))\n",
    "        return 0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Замечения к классу\n",
    "\n",
    "Вообще-то, он страшный. Там смешан словарь и векторизация текстов. Вообще-то, это должно быть два разных класса.\n",
    "\n",
    "С другой стороны, если от всей этой ваашей морфологии нам нужны только векторы, то вроде бы сойдет, даже быстрее должно работать. Беда в том, что есть решения и побыстрее. Так что это просто учебный класс, который показывает как оно может векторизоваться. И чтобы показать, что можно использовать разреженное представление векторов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим как работает новая морфология. Возьмем также один из \"Севастопольских рассказов\" того же автора, чтобы было что использовать при расчете косинусной меры сходства."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "faster3 = FasterMorphology2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/war_and_peace.txt') as fil:\n",
    "    textWP = fil.read()\n",
    "# Выделяем все слова написанные русской кириллицей.\n",
    "words = [w[0] for w in re.findall('([А-ЯЁа-яё]+(-[А-ЯЁа-яё]+)*)', textWP)]\n",
    "newtext = ' '.join(words)\n",
    "\n",
    "with open('data/sebastopol.txt') as fil:\n",
    "    textSb = fil.read()\n",
    "words3 = [w[0].lower() for w in re.findall('([А-ЯЁа-яё]+(-[А-ЯЁа-яё]+)*)', textSb)]\n",
    "newtext3 = ' '.join(words3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим плотное и разреженное представление для двух текстов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vd1 = faster3.vectorizeAsDict(words)\n",
    "vd2 = faster3.vectorizeAsDict(words3)\n",
    "vl1 = faster3.vectorizeAsList(words)\n",
    "vl2 = faster3.vectorizeAsList(words3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим как быстро считается разреженное и плотное предствления."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.14 ms, sys: 8 µs, total: 4.15 ms\n",
      "Wall time: 4.05 ms\n",
      "CPU times: user 2.96 ms, sys: 0 ns, total: 2.96 ms\n",
      "Wall time: 2.96 ms\n",
      "0.9561367264390299 0.9561367264390299\n"
     ]
    }
   ],
   "source": [
    "%time r1 = faster3.cosineSimilarity(vd1, vd2)\n",
    "%time r2 = faster3.cosineSimilarity(vl1, vl2)\n",
    "print(r1, r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "528 15348 18838\n"
     ]
    }
   ],
   "source": [
    "print(vl1.count(0), vl2.count(0), len(vl1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.24 ms, sys: 16 µs, total: 6.26 ms\n",
      "Wall time: 6.26 ms\n",
      "CPU times: user 2.33 ms, sys: 0 ns, total: 2.33 ms\n",
      "Wall time: 2.34 ms\n",
      "0.1572353753052341 0.1572353753052341\n"
     ]
    }
   ],
   "source": [
    "%time r1 = faster3.JaccardCoefficient(vd1, vd2)\n",
    "%time r2 = faster3.JaccardCoefficient(vl1, vl2)\n",
    "print(r1, r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим сколько наши вектора занимают памяти."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "589920\n",
      "153752\n",
      "147552\n",
      "153752\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.getsizeof(vd1))\n",
    "print(sys.getsizeof(vl1))\n",
    "print(sys.getsizeof(vd2))\n",
    "print(sys.getsizeof(vl2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разница по памяти в 4 раза. Попробуем с numpy.array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150816\n"
     ]
    }
   ],
   "source": [
    "va1 = faster3.vectorizeAsArray(words)\n",
    "print(sys.getsizeof(va1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь какой-то небольшой выигрыш. Мораль - проще разработать свой собственный класс для разреженного хранения на основе общего для всех словаря с индексом и двумя массивами для индекса слов конкретного текста и их частот.\n",
    "\n",
    "Хорошо, продолжим с косинусной мерой. Попробуем посчитать сходство с \"Марсианином\" Энди Вейра."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/veyr/index_split_017.xhtml') as fil: # Грузим главу 17, она побольше.\n",
    "    textM17 = fil.read()\n",
    "words4 = [w[0].lower() for w in re.findall('([А-ЯЁа-яё]+(-[А-ЯЁа-яё]+)*)', textM17)]\n",
    "newtext4 = ' '.join(words4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Считаем вектора.\n",
    "vd3 = faster3.vectorizeAsDict(words4)\n",
    "vl3 = faster3.vectorizeAsList(words4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7850065150553005\n",
      "0.7773244182267292\n",
      "CPU times: user 7.77 ms, sys: 0 ns, total: 7.77 ms\n",
      "Wall time: 7.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(faster3.cosineSimilarity(vd1, vd3))\n",
    "print(faster3.cosineSimilarity(vd2, vd3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "CPU times: user 111 µs, sys: 15 µs, total: 126 µs\n",
      "Wall time: 131 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(faster3.cosineSimilarity(vl1, vl3))\n",
    "print(faster3.cosineSimilarity(vl2, vl3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что-то пошло не так. Постараемся понять что именно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_49575/5167731.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msumab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvl1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvl3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mna\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvl3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msuma2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvl1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvl1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mna\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvl3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msumb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvl3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvl3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mna\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvl3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msumab\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuma2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msumb2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_49575/5167731.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msumab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvl1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvl3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mna\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvl3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msuma2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvl1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvl1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mna\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvl3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msumb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvl3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvl3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mna\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvl3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msumab\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuma2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msumb2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "sumab = sum([vl1[na] * vl3[na] for na in range(len(vl3))])\n",
    "suma2 = sum([vl1[na] * vl1[na] for na in range(len(vl3))])\n",
    "sumb2 = sum([vl3[na] * vl3[na] for na in range(len(vl3))])\n",
    "sumab / math.sqrt(suma2 * sumb2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну конечно же! Надо же пересчитать все вектора, а то размерность пространства изменилась! Со словарями такой проблемы не было."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vl1 = faster3.vectorizeAsList(words)\n",
    "vl2 = faster3.vectorizeAsList(words3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7850065150553005\n",
      "0.7773244182267292\n",
      "CPU times: user 8.18 ms, sys: 0 ns, total: 8.18 ms\n",
      "Wall time: 8.62 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(faster3.cosineSimilarity(vl1, vl3))\n",
    "print(faster3.cosineSimilarity(vl2, vl3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(954, 15774, 17675, 19264)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vl1.count(0), vl2.count(0), vl3.count(0), len(vl1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем построить вектора для всего текста \"Марсианина\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "words5 = []\n",
    "for i in range(2, 33):\n",
    "    with open(f'data/veyr/index_split_0{i:0>2}.xhtml') as fil:\n",
    "        textMar = fil.read()\n",
    "    words6 = [w[0].lower() for w in re.findall('([А-ЯЁа-яё]+(-[А-ЯЁа-яё]+)*)', textMar)]\n",
    "    words5 += words6\n",
    "newtext5 = ' '.join(words5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vd4 = faster3.vectorizeAsDict(words5)\n",
    "vl4 = faster3.vectorizeAsList(words5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vl1 = faster3.vectorizeAsList(words)\n",
    "vl2 = faster3.vectorizeAsList(words3)\n",
    "vl3 = faster3.vectorizeAsList(words4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8039345159867894\n",
      "0.8083855950536852\n",
      "0.8813710876467187\n",
      "CPU times: user 12 ms, sys: 0 ns, total: 12 ms\n",
      "Wall time: 11.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(faster3.cosineSimilarity(vd1, vd4))\n",
    "print(faster3.cosineSimilarity(vd2, vd4))\n",
    "print(faster3.cosineSimilarity(vd3, vd4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Да, глава из \"Марсианина\" больше похожа на всё произведение, чем на Толстого. Но общая мера сходства довольно большая. То есть по-хорошему, речь идет в большой степени об одинаковых вещах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8039345159867894\n",
      "0.8083855950536852\n",
      "0.8813710876467187\n",
      "CPU times: user 16.2 ms, sys: 0 ns, total: 16.2 ms\n",
      "Wall time: 15.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(faster3.cosineSimilarity(vl1, vl4))\n",
    "print(faster3.cosineSimilarity(vl2, vl4))\n",
    "print(faster3.cosineSimilarity(vl3, vl4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18310 3490 1589 8010 22238\n",
      "445508 90841\n"
     ]
    }
   ],
   "source": [
    "print(len(vl1) - vl1.count(0), \n",
    "      len(vl1) - vl2.count(0), \n",
    "      len(vl1) - vl3.count(0), \n",
    "      len(vl1) - vl4.count(0), \n",
    "      len(vl1))\n",
    "print(len(words), len(words5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104178\n",
      "8735\n"
     ]
    }
   ],
   "source": [
    "with open('data/war_and_peace3.txt') as fil:\n",
    "    textWP = fil.read()\n",
    "words6 = [w[0] for w in re.findall('([А-ЯЁа-яё]+(-[А-ЯЁа-яё]+)*)', textWP)]\n",
    "print(len(words6))\n",
    "faster4 = FasterMorphology2()\n",
    "vd5 = faster4.vectorizeAsDict(words6)\n",
    "vl5 = faster4.vectorizeAsList(words6)\n",
    "print(len(vl5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ещё два слова про производительность\n",
    "\n",
    "Как уже было сказано выше, вычисление косинуса при помощи SciPy будет быстрее. На самом деле почти всё будет быстрее, чем тот код, что написан выше, но мне хотелось сравнимых решений, демонстрирующих внутреннюю структуру метода.\n",
    "\n",
    "А теперь давайте посмотрим как можно сделать быстрее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим случайный массив\n",
    "arr_size = 1000\n",
    "a = np.random.rand(100)\n",
    "ad = {i:a[i] for i in range(100)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.5 µs ± 86.7 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# Вот так считается часть косинусной меры у нас.\n",
    "s = sum([ad[i]*ad[i] for i in ad.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.97 µs ± 157 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# Используем скалярное произведение векторов для того, чтобы посчитать сумму поэлементных произведений.\n",
    "s = np.dot(np.array(list(ad.values())), np.array(list(ad.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.38 µs ± 57.3 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# Этот небольшой трюк сократит время расчетов в два раза.\n",
    "a2 = np.array(list(ad.values()))\n",
    "s = np.dot(a2, a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "835 ns ± 33.1 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "s = np.dot(a, a) # А если бы мы с самого начала использовали numpy..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>CountVectorizer и TfidfVectorizer</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На самом деле примерно всё то же самое можно селать при помощи класса CountVectorizer из sklearn.feature_extraction.text. При помощи функции <i>fit\\_transform</i> можно получить разреженное представление матрицы частот слов. Основная проблема состоит в том, что индексы в матрице представляют собой индексы в словаре переданных текстов. Сам словарь хранится в свойстве <i>vocabulary\\_</i> и умеет возвращать индекс по слову (но не наоборот)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5)\t10\n",
      "  (0, 6)\t7\n",
      "  (0, 7)\t3\n",
      "  (0, 0)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 9)\t3\n",
      "20342\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "counter = CountVectorizer()\n",
    "# Просим посчитать частоты слов.\n",
    "res = counter.fit_transform([newtext, newtext3, newtext5])\n",
    "# Разреженное представление счетчика.\n",
    "print(res[0,:10])\n",
    "# Можно получить индекс по слову, ...\n",
    "print(counter.vocabulary_.get('левый'))\n",
    "# ... но не наоборот.\n",
    "print(counter.vocabulary_.get(20342))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более того, CountVectorizer просто выделяет подстроки и ничего не знает про морфологию (ее можно правильно прикрутить, но это хлопотное занятие). Зато он умеет выделять n-граммы (n слов идущих подряд (или даже букв)). Помимо этого, можно попросить выдать все подстроки, создав анализатор. И можно сказать как выделять подстроки при помощи регулярного выражения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMeaningfullWords(text: str, morph: MorphAnalyzer):\n",
    "    words = []\n",
    "    tokens = re.findall('[А-Яа-яЁё]+\\-[А-Яа-яЁё]+|[А-Яа-яЁё]+', text)\n",
    "    for t in tokens:\n",
    "        pv = morph.parse(t)\n",
    "        if pv[0].tag.POS in ['ADJF', 'NOUN', 'VERB', 'PRTF', 'GRND']:\n",
    "            words.append(pv[0].normal_form)\n",
    "    return words\n",
    "\n",
    "lemmaCounter = CountVectorizer(ngram_range=(1,3), \n",
    "                               token_pattern=r'[А-Яа-яЁё]+\\-[А-Яа-яЁё]+|[А-Яа-яЁё]+')\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "\n",
    "c = [' '.join(getMeaningfullWords(newtext, morph)),\n",
    "     ' '.join(getMeaningfullWords(newtext3, morph)),\n",
    "     ' '.join(getMeaningfullWords(newtext5, morph))]\n",
    "analyze = lemmaCounter.build_analyzer()\n",
    "res1 = analyze(c[0])\n",
    "res2 = lemmaCounter.fit_transform(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['лев', 'николаевич', 'толстой', 'война', 'мир', 'тот', 'часть', 'первый', 'быть', 'поместье']\n"
     ]
    }
   ],
   "source": [
    "print(res1[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем другой показатель для подсчета важности слов в тексте - $TF*IDF$. Здесь $TF$ - Term Frequency, частота термина в документе, а $IDF$ - Inverted Document Frequency, обратная частота термина в коллекции (количество документов, в которых встречается данный термин).\n",
    "\n",
    "Идея метрики очень проста. Если слово встречается почти во всех документах - его различительная сила очень мала и само слово не является важным. Если слово часто встречается в данном документе, то оно являетсяя важным для него.\n",
    "\n",
    "Метрика считается на коллекции документов для каждого слова, каждого документа. Для расчета меры можно использовать `TfidfVectorizer`, который работает так же как `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmaCounter = TfidfVectorizer(ngram_range=(1,3), \n",
    "                               token_pattern=r'[А-Яа-яЁё]+\\-[А-Яа-яЁё]+|[А-Яа-яЁё]+')\n",
    "\n",
    "c = [' '.join(getMeaningfullWords(newtext, morph)),\n",
    "     ' '.join(getMeaningfullWords(newtext3, morph)),\n",
    "     ' '.join(getMeaningfullWords(newtext5, morph))]\n",
    "analyze = lemmaCounter.build_analyzer()\n",
    "res1 = analyze(c[0])\n",
    "res2 = lemmaCounter.fit_transform(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t0.00011569803023339103\n",
      "  (0, 5)\t0.00011569803023339103\n",
      "  (0, 1)\t0.00011569803023339103\n",
      "  (0, 4)\t0.00011569803023339103\n",
      "  (0, 9)\t0.00023139606046678206\n",
      "  (0, 0)\t0.00011569803023339103\n",
      "  (0, 3)\t0.00011569803023339103\n"
     ]
    }
   ],
   "source": [
    "print(res2[0][0,:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем посмотреть на близость глав \"Марсианина\" при помощи косинусной меры по частотам слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsM = []\n",
    "for i in range(2, 33):\n",
    "    with open(f'data/veyr/index_split_0{i:0>2}.xhtml') as fil:\n",
    "        textWP = fil.read()\n",
    "    words6 = [w[0].lower() for w in re.findall('([А-ЯЁа-яё]+(-[А-ЯЁа-яё]+)*)', textWP)]\n",
    "    wordsM.append(words6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_vects = []\n",
    "\n",
    "for words in wordsM:\n",
    "    m_vects.append(faster3.vectorizeAsDict(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9420221325678382 22 25\n"
     ]
    }
   ],
   "source": [
    "nearest = -1\n",
    "txt1 = -1\n",
    "txt2 = -1\n",
    "for i, vct1 in enumerate(m_vects):\n",
    "    for j, vct2 in enumerate(m_vects):\n",
    "        if vct1 is vct2:\n",
    "            continue\n",
    "        cc = faster3.cosineSimilarity(vct1, vct2)\n",
    "        if cc > nearest:\n",
    "            nearest = cc\n",
    "            txt1 = i\n",
    "            txt2 = j\n",
    "print(nearest, txt1+2, txt2+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь возьмем косинусную меру сходства по результатам TF*IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "textM = []\n",
    "for i in range(2, 33):\n",
    "    with open(f'data/veyr/index_split_0{i:0>2}.xhtml') as fil:\n",
    "        textM.append(fil.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmaCounter = CountVectorizer(ngram_range=(1,3), token_pattern=r'[А-Яа-яЁё]+\\-[А-Яа-яЁё]+|[А-Яа-яЁё]+')\n",
    "#lemmaCounter=TfidfVectorizer(ngram_range=(1,3), token_pattern=r'[А-Яа-яЁё]+\\-[А-Яа-яЁё]+|[А-Яа-яЁё]+')\n",
    "\n",
    "analyze = lemmaCounter.build_analyzer()\n",
    "#res1=analyze(c[0])\n",
    "res2 = lemmaCounter.fit_transform(textM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.85149324]] 15 21\n",
      "CPU times: user 1.43 s, sys: 1.98 ms, total: 1.43 s\n",
      "Wall time: 1.44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nearest = -1\n",
    "txt1 = -1\n",
    "txt2 = -1\n",
    "for i, vct1 in enumerate(res2):\n",
    "    for j, vct2 in enumerate(res2):\n",
    "        if i==j:\n",
    "            continue\n",
    "        cc = cosine_similarity(vct1, vct2)\n",
    "        if cc > nearest:\n",
    "            nearest = cc\n",
    "            txt1 = i\n",
    "            txt2 = j\n",
    "print(nearest, txt1+2, txt2+2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.851493236611201 15 21\n",
      "CPU times: user 19.6 ms, sys: 0 ns, total: 19.6 ms\n",
      "Wall time: 18.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cc2 = cosine_similarity(res2, res2)\n",
    "np.fill_diagonal(cc2, 0.)\n",
    "pos = np.argmax(cc2)\n",
    "print(cc2[pos//res2.shape[0], pos%res2.shape[0]], pos//res2.shape[0]+2, pos%res2.shape[0]+2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
